---
title: "Problem Set #1"
subtitle: "BST 258: Causal Inference -- Theory and Practice"
author: "Kimberly Greco"
date: ""
format:
  pdf:
    documentclass: scrartcl
    papersize: letter
    fontsize: 11pt
    geometry:
      - margin=1in
      - heightrounded
    number-sections: false
    colorlinks: true
    link-citations: true
    callout-appearance: simple
    callout-icon: false
    # figure options
    fig-width: 6
    fig-asp: 0.618
    fig-cap-location: bottom
    # code block options
    code-line-numbers: false
    code-block-bg: false
    highlight-style: nord
bibliography: refs.bib
---

```{r}
#| echo: false
#| message: false
#| label: global-setup
library(here)
```

{{< pagebreak >}}

# Question 2

## Consider a completely randomized experiment (CRE) with $i=1, \ldots, n$ units, $m$ of which are treated, where $A_i$ is 1 when unit $i$ is treated.

## a) What is the marginal distribution of the treatment indicator $A$ ?

:::{.callout-note title="Answer"}
We have $m$ units treated and $n-m$ units untreated. Therefore, the marginal distribution of the treatment indicator $A$ is:

$$
\begin{aligned}
& P\left(A_i=1\right)=\frac{m}{n} \\
& P\left(A_i=0\right)=\frac{n-m}{n}
\end{aligned}
$$
:::

## b) What is the joint distribution of $A_i$ and $A_j$ for two units $i \neq j$ ? (Hint: This amounts to completing a contingency table.)

:::{.callout-note title="Answer"}

The joint distribution of $A_i$ and $A_j$ can be expressed as 2x2 contingency table where $A_i \in \{1,0\}$ are the rows and $A_j \in \{1,0\}$ are the columns. The following probabilites make up the entries of the table:

$$
\begin{aligned}
& P\left(A_i=1, A_j=1\right) =\left(\frac{m}{n}\right)\left(\frac{m-1}{n-1}\right) \\
& P\left(A_i=1, A_j=0\right) =\left(\frac{m}{n}\right)\left(\frac{n-m}{n-1}\right) \\
& P\left(A_i=0, A_j=1\right) =\left(\frac{n-m}{n}\right)\left(\frac{m}{n-1}\right) \\
& P\left(A_i=0, A_j=0\right) = \left(\frac{n-m}{n}\right)\left(\frac{n-1-m}{n-1}\right)
\end{aligned}
$$

Note that because $P\left(A_i=1, A_j=0\right) = P\left(A_i=0, A_j=1\right)$, we have:

$$
\begin{aligned}
& P\left(A_i=1, A_j=1\right) =\left(\frac{m}{n}\right)\left(\frac{m-1}{n-1}\right) \\
& P\left(A_i=1, A_j=0\right) = P\left(A_i=0, A_j=1\right) =\left(\frac{m}{n}\right)\left(\frac{n-m}{n-1}\right) \\
& P\left(A_i=0, A_j=0\right) = \left(\frac{n-m}{n}\right)\left(\frac{n-1-m}{n-1}\right)
\end{aligned}
$$
:::

## c) What are $\mathbb{V}\left(A_i\right)$ and $\operatorname{Cov}\left(A_i, A_j\right)$ for $i \neq j$ ?

:::{.callout-note title="Answer"}
We have $A_i \sim \text{Bernoulli} \left(\frac{m}{n}\right)$

$$
\begin{aligned}
\mathbb{E}[A_i]&=P(A_i=1)=\frac{m}{n} \\
\mathbb{V}(A_i)&=\frac{m}{n}\left(1-\frac{m}{n}\right)
\\ \\
\operatorname{Cov}\left(A_i, A_j\right) &= \mathbb{E}\left[A_i A_j\right]-\mathbb{E}[A_i] \mathbb{E}\left[A_j\right] \\
&= P(A_iA_j=1) - P(A_i=1) P(A_j=1) \\
& = \left(\frac{m}{n}\right)\left(\frac{m-1}{n-1}\right)-\left(\frac{m}{n}\right)^2
\end{aligned}
$$
:::

## d) The sample Average Treatment Effect on the Treated (ATT) is $\theta^{\text {ATT }}=\frac{1}{m} \sum_{i=1}^n A_i\left(Y_i(1)-\right.$ $\left.Y_i(0)\right)$. What is the sample ATT in expectation?

:::{.callout-note title="Answer"}
$$
\begin{aligned}
\theta^{A T T} & =\frac{1}{m} \sum_{i=1}^n A_i\left(Y_i(1)-Y_i(0)\right) \\
\mathbb{E}\left[\theta^{A T T}\right] & =\mathbb{E}\left[\frac{1}{m} \sum_{i=1}^n A_i\left(Y_i(1)-Y_i(0)\right)\right] \\
& =\frac{1}{m} \sum_{i=1}^n \mathbb{E}\left[A_i\left(Y_i(1)-Y_i(0)\right)\right] \\
& = \mathbb{E}[Y(1)-Y(0)|A=1] \quad \text{since } A_i \text{ is binary}
\end{aligned}
$$
:::

{{< pagebreak >}}

# Question 3

## Consider an additive treatment effect model, i.e., $\theta=Y_i(1)-Y_i(0)$ for all $i$, so $Y_i(1)=Y_i(0)+\theta$. Show that $\mathbb{V}\left(Y_i(1)\right)=\mathbb{V}\left(Y_i(0)\right)$ and that the correlation $\rho\left(Y_i(1), Y_i(0)\right)=1$, where expectations are sample expectations, i.e., $\left.\mathbb{E}\left[Y_i(1)\right]:=\frac{1}{n} \sum_{i=1}^n Y_i(1)\right)$.

:::{.callout-note title="Answer"}
By definition of variance,
$$
\begin{aligned}
\mathbb{V}\left(Y_i(1)\right)&=\mathbb{E}\left[\left(Y_i(1)-\mathbb{E}\left[Y_i(1)\right]\right)^2\right]\\
&=\mathbb{E}\left[\left(Y_i(0)+\theta-\mathbb{E}\left[Y_i(0)+\theta\right]\right)^2\right]\\
&=\mathbb{E}\left[\left(Y_i(0)+\theta-\mathbb{E}\left[Y_i(0)\right]-\theta\right)^2\right]\\
&=\mathbb{E}\left[\left(Y_i(0)-\mathbb{E}\left[Y_i(0)\right]\right)^2\right]\\
&=\mathbb{V}\left(Y_i(0)\right)
\end{aligned}
$$

By definition of correlation, 
$$
\rho\left(Y_i(1), Y_i(0)\right)=\frac{\operatorname{Cov}\left(Y_i(1), Y_i(0)\right)}{\sqrt{\mathbb{V}(Y_i(1))\mathbb{V}(Y_i(0))}}
$$

We calculate covariance in the numerator as:
$$
\begin{aligned}
\operatorname{Cov}\left(Y_i(1), Y_i(0)\right)&=\mathbb{E}\left[\left(Y_i(1)-\mathbb{E}\left[Y_i(1)\right]\right)\left(Y_i(0)-\mathbb{E}\left[Y_i(0)\right]\right)\right]
\\
&=\mathbb{E}\left[\left(\left(Y_i(0)+\theta\right)-\mathbb{E}\left[Y_i(0)+\theta\right]\right)\left(Y_i(0)-\mathbb{E}\left[Y_i(0)\right]\right)\right]\\
&=\mathbb{E}\left[\left(\left(Y_i(0)+\theta\right)-\mathbb{E}\left[Y_i(0)\right]-\theta\right)\left(Y_i(0)-\mathbb{E}\left[Y_i(0)\right]\right)\right]\\
&=\mathbb{E}\left[\left(Y_i(0)-\mathbb{E}\left[Y_i(0)\right]\right)^2\right]\\
&=\mathbb{V}\left(Y_i(0)\right)
\end{aligned}
$$

Given $\mathbb{V}\left(Y_i(1)\right)=\mathbb{V}\left(Y_i(0)\right)$ from above, the equation simplifies to:
$$
\rho\left(Y_i(1), Y_i(0)\right)=\frac{\mathbb{V}\left(Y_i(0)\right)}{\mathbb{V}\left(Y_i(0)\right)}=1
$$

:::

{{< pagebreak >}}


# Question 4

## It's tea time: (a hologram of) R.A. Fisher places eight cups of tea (with milk) in front of you and asks you to identify which cups had tea poured before milk and vice-versa. Prior to giving you the cups of tea, Fisher poured milk before tea in four of them and tea before milk in the other four. The ordering in which the cups have been served is random. What is the probability that you correctly guess 0,1,2,3, or 4 of all of the cups that had tea poured first?

:::{.callout-note title="Answer"}

Let $X$ be a random variable representing the the number of cups correctly identified as having tea poured first.

&nbsp;

$X \sim$ Hypergeometric $(N=8, K=4, n=4)$, where:

$$
P(X=x)=\frac{\left(\begin{array}{c}
K \\
x
\end{array}\right) \cdot\left(\begin{array}{c}
N-K \\
n-x
\end{array}\right)}{\left(\begin{array}{l}
N \\
n
\end{array}\right)}
$$

* $N=8$ is the total number of cups (population size)
* $K=4$ is the number cups with tea poured first (successes in the population)
* $n=4$ is the number of guesses
* $k$ is the number of observed correct guesses

The probability of correctly identifying $x \in \{0,1,2,3,4\}$ cups that had tea poured first is given by:

```{R}

# Hypergeometric parameters
N <- 8  
K <- 4 
n <- 4  

# Outcome vector
k <- 0:4 

# Hypergeometric density
probabilities <- dhyper(k, K, N-K, n)
names(probabilities) <- k
probabilities

```
:::

{{< pagebreak >}}

# Question 5

The table below displays the success rates of two distinct, investigational treatments for kidney stones, labeled as $A$ and $B$. A study enrolls $n=700$ participants, assigning $n=350$ individuals to either of the treatment arms, $A$ and $B$. To summarize data from the study, individuals' kidney stones are categorized as either small or large, and Table 1 is constructed to summarize the success rates of each of the two treatments.

**Table 1**: Success rates in arms $A$ and $B$ versus kidney stone size

$$
\begin{tabular}{lll}
\hline & Treatment A & Treatment B \\
\hline Small Stones & $93 \%(81 / 87)$ & $87 \%(234 / 270)$ \\
Large Stones & $73 \%(192 / 263)$ & $69 \%(55 / 80)$ \\
Both & $78 \%(273 / 350)$ & $83 \%(289 / 350)$ \\
\hline
\end{tabular}
$$

Studying Table 1, your colleague remarks at the discrepancy between the superior overall success rate of treatment $B$ and its relatively lower success (versus treatment $A$ ) when stratifying cases by kidney stone size.


## a) Describe possible factors that might have contributed to this seemingly contradictory result.

:::{.callout-note title="Answer"}
1. Heterogeneity in case distribution: We see based on the table that treatment $A$ was used to treat a higher proportion of large stones (263/350 receiving treatment $A$ had large stones) while treatment $B$ was used to treat a higher proportion of small stones (270/350 recieveing treatment $B$ had small stones). If larger stones respond better to treatment than smaller stones, this disparity could lead to misleading conclusions about the effectiveness of treatment $A$ compared to treatment $B$ when looking at stratified success rates.
2. Heterogeneity in baseline characteristics: Without the assurance of randomization in the experiment, there's a significant risk of imbalances in baseline characteristics between the treatment groups. If these characteristics influence both the choice of treatment and its success, such imbalances could introduce bias into the observed success rates and affect the validity of the conclusions drawn from the data.
:::

## b) Alarmed by this discrepancy, your colleague asks you to further segment the results by reported gender. This newly refined look at the data suggests that for both small and large kidney stones, treatment $B$ is consistently more effective than treatment $A$ across all genders. Construct a hypothetical (i.e., candidate) table that illustrates this (ensure that your candidate table is consistent with the information given in Table 1).

:::{.callout-note title="Answer"}

Below, Table 2 demonstrates that when the data is further stratified by gender, a consistently better success rate is observed for treatment $B$ across both males (94% in $B$ vs. 84% in $A$) and females (77% in $B$ vs. 74% in $A$).

&nbsp;

**Table 2**: Success rates in arms $A$ and $B$ versus kidney stone size further stratified by gender

$$
\begin{array}{llll}
\hline \text { Kidney Stone Size } & \text { Gender } & \text { Treatment A } & \text { Treatment B } \\
\hline \text { Small Stones } & \text { Male } & 93 \%(37 / 40) & 95 \%(95 / 100) \\
\text { Small Stones } & \text { Female } & 94 \%(44 / 47) & 82 \%(139 / 170) \\
\text { Large Stones } & \text { Male } & 80 \%(80 / 100) & 90 \%(18 / 20) \\
\text { Large Stones } & \text { Female } & 69 \%(112 / 163) & 62 \%(37 / 60) \\
\text { Both } & \text { Male } & \mathbf{84 \% ( 117 / 140 )} & \mathbf{94 \% ( 113 / 120 )} \\
\text { Both } & \text { Female } & \mathbf{7 4 \% ( 156 / 210 )} & \mathbf{77 \% ( 176 / 230 )} \\
\hline
\end{array}
$$
:::

## c) What is this phenomenon (it has a name)? What are its broader implications and significance in the interpretation of data?

:::{.callout-note title="Answer"}

We've demonstrated that when we further stratify the data by gender, we find an opposite trend in treatment effect compared to what we observed in the overall data. This phenomenon is commonly known as Simpson's Paradox. It highlights that (1) conclusions based on aggregated data can be deceptive and might not tell the full story, and (2) it's essential to consider stratification by relevant covariates early in the data analysis process to mitigate opportunities for misinterpretation down the road.

:::

{{< pagebreak >}}

# Question 6

In the middle of a long day, you decide to take a short coffee break. As you meander towards the nearest cafe, a colleague spots you and asks for your help in evaluating the results from a completely randomized experiment (CRE) that they have designed. They've heard that Neyman and Fisher disagreed on the appropriate type of null hypothesis upon which to focus, and they'd like your help in better understanding which — the weak or sharp null-would be most appropriate for the scientific question motivating their experiment. Pressed for time (your day isn't getting shorter), you explain that, in the context of their CRE, the difference-in-means estimator is unbiased for the average treatment effect (ATE):
$$
\widehat{\psi}^{\mathrm{ATE}}=\frac{1}{n_1} \sum_{i=1}^n A_i Y_i-\frac{1}{n_0} \sum_{i=1}^n\left(1-A_i\right) Y_i,
$$
in which $A_i$ indicates the treatment status for the $\mathrm{i}^{\text {th }}$ individual and $Y_i$ their outcome; note that $n_1=\sum_{i=1}^n A_i$ and $n_0=\sum_{i=1}^n\left(1-A_i\right)$. Recall the sharp null hypothesis suggested by Fisher:
$$
H_0^{\text {sharp }}: Y_i(1)-Y_i(0)=0 \forall i=1, \ldots, n,
$$
which states that the potential outcomes do not differ across the treatment conditions $A \in\{0,1\}$; meanwhile, the weak null hypothesis championed instead by Neyman:
$$
H_0^{\text {weak }}: \mathbb{E}\left[Y_i(1)\right]=\mathbb{E}\left[Y_i(0)\right],
$$
which states that there is no effect of treatment in expectation, i.e., the mean potential outcomes do not differ.

To illustrate the difference between these null hypotheses for your colleague, you decide to compare the two in a simulation experiment. Note that Fisher's (sharp) null implies Neyman's (weak) null - no difference in the potential outcomes for all $i=1, \ldots, n$ means that there must be no difference in expectation. While Fisher's null implies Neyman's null, the implication need not run in reverse. In your simulation experiment, you will evaluate this - that if there is evidence against Neyman's null, then there should also be evidence against Fisher's. 

Conduct a simulation study with the following specifications:

1. Set $n_1=n_0=\{10,25,50,100,250\}$.
2. Independently sample $Y_i(1) \sim \mathrm{N}\left(\mu=1 / 10, \sigma^2=1 / 16\right)$ and $Y_i(0) \sim \mathrm{N}\left(\mu=0, \sigma^2=1 / 16\right)$ for $i=1, \ldots, n=n_1+n_0$. Note that the treatment effect is $\theta^{\mathrm{ATE}}=\mathbb{E}[Y(1)-Y(0)]=1 / 10$.
3. Conduct $n_{\text {sim }}=1000$ completely randomized treatment assignments, i.e., sample $A \sim \operatorname{Bern}(p=0.5)$, using the treatment assignment to reveal the potential outcomes.
4. Using the difference-in-means test statistic, evaluate evidence of there being a treatment effect, based on the weak and sharp null hypotheses, at significance level $\alpha=0.05$. To test the sharp null, conduct at least $B=10000$ repetitions in generating the test statistic's null distribution.
5. Compute the power for both tests at each of the three sample sizes $n=\{20,50,100,200,500\}$ and display your results in a figure.
6. Comment on your findings.

:::{.callout-note title="Answer"}

```{R, message=F, warning=F}
library(fastverse)
library(tidyverse)
library(patchwork)
library(ggsci)

# Simulation parameters
n1 <- c(10, 25, 50, 100, 250)
n0 <- c(10, 25, 50, 100, 250)
n_sim <- 1000

# Permutation parameters
B <- 100L           # Set number of permutations
alpha <- 0.05       # Set significance level

```

```{R}

# Simulation function
sim <- function(n1, n0, B, alpha) {
  
  # Placeholder for p-values
  pval_sharp <- numeric(n_sim)
  pval_weak <- numeric(n_sim)
  
  for (i in 1:n_sim) {
    # Randomize treatment
    A <- sample(c(rep(1, n1), rep(0, n0)))
    
    # Generate outcomes
    Y1 <- rnorm(n1, mean = 1/10, sd = sqrt(1/16))
    Y0 <- rnorm(n0, mean = 0, sd = sqrt(1/16))
    Y <- A * Y1 + (1 - A) * Y0
    
    # Observed value of difference-in-means test statistic
    t_obs <- abs(mean(Y[A == 1]) - mean(Y[A == 0]))
    
    # Testing under the sharp null
    # Generate null distribution of test statistic under sharp null hypothesis
    # Permute treatment assignment B times
    t_null <- lapply(seq_len(B), function(j) {
      # Randomly permute treatment assignments
      A_perm <- sample(A)
      # Calculate test statistic under the permuted treatment assignments
      t_perm <- abs(mean(Y[A_perm == 1]) - mean(Y[A_perm == 0])) 
      # Return test statistic for each permutation
      return(t_perm)
      })
    
    t_null <- do.call(c, t_null)
    pval_sharp[i] <- mean(t_null >= t_obs)
    
    # Testing under the weak null
    pval_weak[i] <- t.test(Y ~ A)$p.value
  }
  
  # Power calculation
  power_sharp <- mean(pval_sharp < alpha)
  power_weak <- mean(pval_weak < alpha)
  
  return(list(power_sharp = power_sharp, power_weak = power_weak))
}

results <- Map(sim, n1, n0, MoreArgs = list(B = B, alpha = alpha))
  power_sharp <- sapply(results, function(x) x$power_sharp)
  power_weak <- sapply(results, function(x) x$power_weak)
```

```{R}

ggdata <- data.frame(
  SampleSize = rep(n1, times = 2), 
  Power = c(power_sharp, power_weak), 
  Hypothesis = factor(rep(c("Sharp Null", "Weak Null"), each = length(n1))) 
)

ggplot(ggdata, aes(x = SampleSize, y = Power, color = Hypothesis, group = Hypothesis)) +
  geom_line() + 
  geom_point() + 
  scale_color_manual(values = c("Sharp Null" = "cornflowerblue", "Weak Null" = "coral")) +
  theme_minimal() +
  labs(x = "Sample Size", y = "Power", 
       title = "Power of Tests for Sharp vs. Weak Null") 

```

Interpretation: The weak null should always be more powerful than the sharp null (since the sharp null is contained within the weak null). That said, we are not surprised to see that the power is very similar between the hypotheses because...

:::

{{< pagebreak >}}

# Question 7

## Consider a completely randomized experiment (CRE) having enrolled $i=1, \ldots, n$ study units, $m$ of which receive the treatment condition. Let $A_i$ be the indicator of the $\mathrm{i}^{\text {th }}$ unit having received the treatment, and, further, define $\bar{Y}_1=\frac{1}{m} \sum_{i=1}^n A_i Y_i$ be the average outcome of the treated units, and similarly define $\bar{Y}_0$. You have been tasked with estimating the average treatment effect (ATE), for which it suffices to solve the following least squares program:
$$
\min _{\alpha, \beta} \frac{1}{2 n} \sum_{i=1}^n\left(Y_i-\alpha-\beta A_i\right)^2
$$

## a) Solve the linear program in $(\alpha, \beta)$ to obtain solutions for each of the two parameters, denoting these $(\hat{\alpha}, \hat{\beta})$.

:::{.callout-note title="Answer"}

$$
\begin{aligned}
\frac{\partial}{\partial \alpha} \frac{1}{2 n} \sum_{i=1}^n\left(Y_i-\alpha-\beta A_i\right)^2 &=
\frac{1}{2 n} \sum_{i=1}^n2\left(Y_i-\alpha-\beta A_i\right)\left(-1\right) \\ 
&=-\frac{1}{ n}\left( \sum_{i=1}^nY_i-n\alpha-\sum_{i=1}^n\beta A_i\right)\\ 
&=\alpha-\frac{1}{ n}\sum_{i=1}^n\left( Y_i-\beta A_i\right)\\ 
0&=\hat{\alpha}-\frac{1}{ n}\sum_{i=1}^n\left( Y_i-\hat{\beta} A_i\right)\\ \\
\hat{\alpha}&=\frac{1}{ n}\sum_{i=1}^n\left( Y_i-\hat{\beta} A_i\right)\\ 
\end{aligned}
$$

$$
\begin{aligned}
\frac{\partial}{\partial \beta} \frac{1}{2 n} \sum_{i=1}^n\left(Y_i-\alpha-\beta A_i\right)^2
&=\frac{1}{2 n} \sum_{i=1}^n2\left(Y_i-\alpha-\beta A_i\right)\left(-A_i\right) \\ 
&=-\frac{1}{ n} \sum_{i=1}^n \left(Y_iA_i-\alpha A_i-\beta A_i^2\right)\\ 
&=-\frac{1}{ n} \sum_{i=1}^n \left(Y_iA_i-\alpha A_i\right)+\frac{1}{ n}\sum_{i=1}^n\beta A_i^2\\ \\ 
&0=-\frac{1}{ n} \sum_{i=1}^n \left(Y_iA_i-\hat{\alpha} A_i\right)+\hat{\beta}\frac{1}{ n}\sum_{i=1}^nA_i^2 \\ 
-\hat{\beta}\frac{1}{ n}\sum_{i=1}^nA_i^2
&=-\frac{1}{ n} \sum_{i=1}^n \left(Y_iA_i-\hat{\alpha} A_i\right)\\ \\
\hat{\beta}
&=\frac{ \sum_{i=1}^n \left(Y_iA_i-\hat{\alpha} A_i\right)}{\sum_{i=1}^nA_i^2} = \frac{ \sum_{i=1}^n A_i\left(Y_i-\hat{\alpha} \right)}{m}
\end{aligned}
$$
:::

## b) Is $\hat{\beta}$ a valid estimator of the ATE? Explain your answer.

:::{.callout-note title="Answer"}

$\hat{\beta}$ represents the estimated average treatment effect of the intervention (difference in the expected outcome between treated and untreated units, after adjusting for the baseline level of the outcome represented by $\hat{\alpha}$).

* Numerator: $\sum_{i=1}^n A_i\left(Y_i-\hat{\alpha}\right)$ sums up the adjusted outcomes (adjusted by subtracting $\hat{\alpha}$) for the treated units only. This adjustment removes the estimated baseline level of the outcome, isolating the effect of the treatment.
* Denominator: $m$ normalizes the sum by the number of treated units.

:::








